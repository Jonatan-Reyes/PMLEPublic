{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.1625\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load ResNet50 with random initialization\n",
    "res1 = resnet50(weights=None)\n",
    "res1.fc = nn.Linear(res1.fc.in_features, 10)  # Update output layer for 10 classes\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),               # Resize to 224x224\n",
    "    transforms.Grayscale(3),                     # Convert 1 channel to 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),  # Dequantize pixel values\n",
    "    transforms.Lambda(lambda x: (x - 0.5) * 2.0)  # Map from [0,1] -> [-1,1]\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "batch_size = 32\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./mnist_data', download=True, train=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = res1.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "#Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader_train:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the device\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader_train):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"10epochresnet11222.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62780/934244244.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load( \"10epochresnet1.pt\"))\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  # Update output layer for 10 classes\n",
    "\n",
    "model.load_state_dict(torch.load( \"10epochresnet1.pt\"))\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9910/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from torcheval.metrics.functional import multiclass_f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),               # Resize to 224x224\n",
    "    transforms.Grayscale(3),                     # Convert 1 channel to 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),  # Dequantize pixel values\n",
    "    transforms.Lambda(lambda x: (x - 0.5) * 2.0)  # Map from [0,1] -> [-1,1]\n",
    "])\n",
    "batch_size =32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./mnist_data', download=True, train=False, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "acc_history =[]\n",
    "def test(epoch):\n",
    "    model.eval() # set model in inference mode (need this because of dropout)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in dataloader_test:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(dataloader_test) # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(dataloader_test.dataset)\n",
    "    acc_history.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(dataloader_test.dataset),\n",
    "        accuracy))\n",
    "test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62780/2323534067.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  dummy =torch.FloatTensor([np.zeros((3,224, 224))]).to(device)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dummy =torch.FloatTensor([np.zeros((3,224, 224))]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "a = softmax(model(dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.999999999999999\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import log\n",
    "from numpy import mean\n",
    "from numpy import exp\n",
    " \n",
    "\n",
    " \n",
    "# calculate the inception score for p(y|x)\n",
    "def inceptionscore(p_yx, eps=1E-16):\n",
    "\t# calculate p(y)\n",
    "\tp_y = expand_dims(p_yx.mean(axis=0), 0)\n",
    "\t# kl divergence for each image\n",
    "\tkl_d = p_yx * (log(p_yx + eps) - log(p_y + eps))\n",
    "\t# sum over classes\n",
    "\tsum_kl_d = kl_d.sum(axis=1)\n",
    "\t# average over images\n",
    "\tavg_kl_d = mean(sum_kl_d)\n",
    "\t# undo the logs\n",
    "\tis_score = exp(avg_kl_d)\n",
    "\treturn is_score\n",
    " \n",
    "# conditional probabilities for high quality images\n",
    "p_yx = asarray([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "score = inceptionscore(p_yx)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inceptionscore(a.cpu().detach().numpy())    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
